{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_google_vertexai in c:\\program files\\python312\\lib\\site-packages (from -r ../requirements.txt (line 1)) (2.0.14)\n",
      "Requirement already satisfied: langgraph in c:\\program files\\python312\\lib\\site-packages (from -r ../requirements.txt (line 2)) (0.3.5)\n",
      "Requirement already satisfied: google-cloud-aiplatform in c:\\program files\\python312\\lib\\site-packages (from -r ../requirements.txt (line 3)) (1.82.0)\n",
      "Collecting duckdb (from -r ../requirements.txt (line 4))\n",
      "  Downloading duckdb-1.2.1-cp312-cp312-win_amd64.whl.metadata (995 bytes)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in c:\\program files\\python312\\lib\\site-packages (from langchain_google_vertexai->-r ../requirements.txt (line 1)) (2.19.0)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in c:\\program files\\python312\\lib\\site-packages (from langchain_google_vertexai->-r ../requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\program files\\python312\\lib\\site-packages (from langchain_google_vertexai->-r ../requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.31 in c:\\program files\\python312\\lib\\site-packages (from langchain_google_vertexai->-r ../requirements.txt (line 1)) (0.3.41)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.9 in c:\\program files\\python312\\lib\\site-packages (from langchain_google_vertexai->-r ../requirements.txt (line 1)) (2.9.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\program files\\python312\\lib\\site-packages (from langgraph->-r ../requirements.txt (line 2)) (2.0.16)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in c:\\program files\\python312\\lib\\site-packages (from langgraph->-r ../requirements.txt (line 2)) (0.1.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\program files\\python312\\lib\\site-packages (from langgraph->-r ../requirements.txt (line 2)) (0.1.53)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in c:\\program files\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (2.24.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (2.37.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (1.26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (5.29.2)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (3.30.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (1.14.1)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\program files\\python312\\lib\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (2.0.6)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python312\\lib\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: docstring-parser<1 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-aiplatform->-r ../requirements.txt (line 3)) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\program files\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\program files\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\program files\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\program files\\python312\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\program files\\python312\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\program files\\python312\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (2.4.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.14.0 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (0.14.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\program files\\python312\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai->-r ../requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: anyio in c:\\program files\\python312\\lib\\site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai->-r ../requirements.txt (line 1)) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai->-r ../requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\program files\\python312\\lib\\site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai->-r ../requirements.txt (line 1)) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\program files\\python312\\lib\\site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai->-r ../requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->langchain_google_vertexai->-r ../requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\program files\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.31->langchain_google_vertexai->-r ../requirements.txt (line 1)) (0.1.134)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\program files\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.31->langchain_google_vertexai->-r ../requirements.txt (line 1)) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\program files\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.31->langchain_google_vertexai->-r ../requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\program files\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.31->langchain_google_vertexai->-r ../requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in c:\\program files\\python312\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph->-r ../requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\program files\\python312\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph->-r ../requirements.txt (line 2)) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0,>=2.9->langchain_google_vertexai->-r ../requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\program files\\python312\\lib\\site-packages (from pydantic<3.0,>=2.9->langchain_google_vertexai->-r ../requirements.txt (line 1)) (2.23.4)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from shapely<3.0.0dev->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (1.26.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\program files\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.31->langchain_google_vertexai->-r ../requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\program files\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.31->langchain_google_vertexai->-r ../requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\program files\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->-r ../requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain_google_vertexai->-r ../requirements.txt (line 1)) (1.3.0)\n",
      "Downloading duckdb-1.2.1-cp312-cp312-win_amd64.whl (11.4 MB)\n",
      "   ---------------------------------------- 0.0/11.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.4 MB 2.6 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/11.4 MB 2.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/11.4 MB 2.4 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/11.4 MB 2.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.6/11.4 MB 2.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/11.4 MB 2.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.8/11.4 MB 2.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.1/11.4 MB 2.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.2/11.4 MB 2.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/11.4 MB 2.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.4/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.5/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.7/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.8/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.9/11.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.1/11.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.2/11.4 MB 2.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.5/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.7/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.8/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.8/11.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.0/11.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.1/11.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.2/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.3/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.5/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.7/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.8/11.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.9/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.1/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.4/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.6/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.8/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.0/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.2/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.4/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.7/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.9/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.1/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.2/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.2/11.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.5/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.6/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.8/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.9/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.9/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.9/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.9/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.9/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.9/11.4 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.7/11.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.2/11.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.9/11.4 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.2/11.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.9/11.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.6/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.8/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.9/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.4/11.4 MB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: duckdb\n",
      "Successfully installed duckdb-1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import duckdb\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the local dataset directory and table names.\n",
    "DATASET_DIR = \"../data/choc_ai_dataset\"  \n",
    "TABLES = [\"menu\", \"order\", \"order_item\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file at: c:\\Users\\user\\Documents\\Learning\\Development\\agentic-workflow\\projects\\agentic-workflow-for-BigQuery-data-using-langraph-and-gemini\\data\\choc_ai_dataset\\menu.csv\n",
      "Loaded table menu from ../data/choc_ai_dataset\\menu.csv.\n",
      "\n",
      "Looking for file at: c:\\Users\\user\\Documents\\Learning\\Development\\agentic-workflow\\projects\\agentic-workflow-for-BigQuery-data-using-langraph-and-gemini\\data\\choc_ai_dataset\\order.csv\n",
      "Loaded table order from ../data/choc_ai_dataset\\order.csv.\n",
      "\n",
      "Looking for file at: c:\\Users\\user\\Documents\\Learning\\Development\\agentic-workflow\\projects\\agentic-workflow-for-BigQuery-data-using-langraph-and-gemini\\data\\choc_ai_dataset\\order_item.csv\n",
      "Loaded table order_item from ../data/choc_ai_dataset\\order_item.csv.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to DuckDB (in-memory; you can also persist to a file if needed)\n",
    "conn = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# Loading CSV files as tables if they don't already exist.\n",
    "for table in TABLES:\n",
    "    table_name = table  # Using table name directly\n",
    "    csv_path = os.path.join(DATASET_DIR, f\"{table}.csv\")\n",
    "\n",
    "    # For, debugging: Print absolute path and check if file exists.\n",
    "    abs_path = os.path.abspath(csv_path)\n",
    "    print(f\"Looking for file at: {abs_path}\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"File {abs_path} does not exist. Check your directory structure.\")\n",
    "        continue\n",
    "    \n",
    "    # Fetching all tables and filter for our table name.\n",
    "    existing_tables = [row[0] for row in conn.execute(\"SHOW TABLES\").fetchall()]\n",
    "    \n",
    "    if table_name in existing_tables:\n",
    "        print(f\"Table {table_name} already exists. Skipping load.\")\n",
    "    else:\n",
    "        # DuckDB can automatically detect CSV schema via read_csv_auto.\n",
    "        conn.execute(f'CREATE TABLE \"{table_name}\" AS SELECT * FROM read_csv_auto(\\'{csv_path}\\')')\n",
    "        print(f\"Loaded table {table_name} from {csv_path}.\")\n",
    "        print()\n",
    "\n",
    "def get_schema() -> str:\n",
    "    \"\"\"Retrieves and returns the DuckDB schema for all tables as a JSON string.\"\"\"\n",
    "    schema_info = {}\n",
    "    \n",
    "    # Listing all tables.\n",
    "    tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "    \n",
    "    for (table,) in tables:\n",
    "        # Getting table info using PRAGMA.\n",
    "        columns = conn.execute(f\"PRAGMA table_info('{table}')\").fetchall()\n",
    "        \n",
    "        # Each row: (cid, name, type, notnull, dflt_value, pk)\n",
    "        schema_info[table] = [{\"name\": col[1], \"type\": col[2]} for col in columns]\n",
    "    \n",
    "    return json.dumps(schema_info, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def execute_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against DuckDB and return the results as a JSON string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Executing the query and fetch all rows.\n",
    "        result = conn.execute(query).fetchall()\n",
    "        \n",
    "        # Getting column names from the cursor description.\n",
    "        columns = [desc[0] for desc in conn.description]\n",
    "        \n",
    "        # Combining rows with column names.\n",
    "        data = [dict(zip(columns, row)) for row in result]\n",
    "    \n",
    "        return json.dumps(data, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_message = f\"DuckDB Error: {str(e)}\"\n",
    "    \n",
    "        return json.dumps({\"error\": error_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"Represents the final answer submitted by the agent.\"\"\"\n",
    "    final_answer: str = Field(..., description=\"The final answer to submit to the user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "# Ensure that your OpenAI API key is set in the environment variable OPENAI_API_KEY,\n",
    "# or pass it directly here using openai_api_key=\"YOUR_API_KEY\"\n",
    "data_llm_with_tools = ChatOpenAI(model_name=MODEL, openai_api_key=os.getenv(\"OPENAI_API_KEY\")).bind_tools([execute_query_tool, SubmitFinalAnswer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample system message; note the schema placeholder will be replaced dynamically.\n",
    "sys_message_template = \"\"\"\n",
    "You are an expert in answering questions about the local dataset stored in DuckDB.\n",
    "\n",
    "Your job is to execute the relevant SQL statements against the tables to get the best answer.\n",
    "The user is only interested in seeing the final result.\n",
    "\n",
    "1. If the user request is reasonable and compatible with the schema, YOU MUST FIRST call the `execute_query_tool`\n",
    "   to get the result.\n",
    "   When generating the SQL query:\n",
    "   - Use meaningful aliases for column names.\n",
    "   - Limit results to 5 rows (unless specified). Order results for clarity.\n",
    "   - Select only necessary columns; avoid SELECT *.\n",
    "   - Use valid DuckDB SQL (no escape characters).\n",
    "   - Use only SELECT statements (no DML).\n",
    "\n",
    "2. Call the `execute_query_tool` to execute the generated SQL query. If the query fails, analyze the error message and attempt to correct the SQL.\n",
    "   If correction is not possible, inform the user of the error and its likely cause.\n",
    "\n",
    "3. Only once you have the result from DuckDB, call the `SubmitFinalAnswer` tool to present the final results to the user and terminate the conversation.\n",
    "\n",
    "You will use the following schema for all queries and all SQL must conform to this schema:\n",
    "{schema}\n",
    "\n",
    "EXAMPLE:\n",
    "If a user asks: \"Which is the most expensive item on the menu?\"\n",
    "You should:\n",
    "1. Call execute_query_tool to execute SQL such as:\n",
    "   SELECT menu_name, menu_price AS price FROM menu ORDER BY menu_price DESC LIMIT 1\n",
    "2. Then call SubmitFinalAnswer to respond to the user with the result.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"Defines the workflow state.\"\"\"\n",
    "    messages: list[AnyMessage]\n",
    "    dataset_schema: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_node(state: dict) -> dict:\n",
    "    \"\"\"Retrieves the schema from DuckDB and stores it in the state.\"\"\"\n",
    "    if not state.get(\"dataset_schema\"):\n",
    "        schema = get_schema()\n",
    "    \n",
    "        return {\"dataset_schema\": schema, \"messages\": [AIMessage(content=\"Schema retrieved from DuckDB.\")]}\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=\"Schema retrieved from memory.\")]}\n",
    "\n",
    "# def data_chatbot_node(state):\n",
    "#     schema = state[\"dataset_schema\"]\n",
    "#     messages = [SystemMessage(content=sys_message_template.format(schema=schema))] + state[\"messages\"]\n",
    "\n",
    "#     # Ensuring the message sequence is correct\n",
    "#     for i in range(len(messages) - 1):\n",
    "\n",
    "#         if hasattr(messages[i], 'role') and messages[i].role == 'tool_calls':\n",
    "\n",
    "#             if not hasattr(messages[i + 1], 'role') or messages[i + 1].role != 'tool':\n",
    "#                 raise ValueError(\"A 'tool' message must follow a 'tool_calls' message.\")\n",
    "\n",
    "#     response = data_llm_with_tools.invoke(messages)\n",
    "\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "def data_chatbot_node(state):\n",
    "    schema = state[\"dataset_schema\"]\n",
    "    \n",
    "    # Creating a new list with just the system message to start\n",
    "    filtered_messages = [SystemMessage(content=sys_message_template.format(schema=schema))]\n",
    "    \n",
    "    # Processing the existing messages to ensure proper tool call structure\n",
    "    state_messages = state[\"messages\"]\n",
    "    \n",
    "    # Looping through the state messages and build a properly structured sequence\n",
    "    tool_calls_message = None\n",
    "    \n",
    "    for msg in state_messages:\n",
    "        \n",
    "        # If we find a message with tool_calls, store it and wait for the corresponding tool response\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            tool_calls_message = msg\n",
    "            filtered_messages.append(msg)\n",
    "        \n",
    "        # If this is a tool response message, make sure it follows a tool_calls message\n",
    "        elif hasattr(msg, 'role') and msg.role == 'tool':\n",
    "        \n",
    "            # Only add tool messages if they have a preceding tool_calls message\n",
    "            if tool_calls_message is not None:\n",
    "                filtered_messages.append(msg)\n",
    "                tool_calls_message = None  # Reset after adding the pair\n",
    "        else:\n",
    "            # For regular messages (human, AI), just add them normally\n",
    "            filtered_messages.append(msg)\n",
    "    \n",
    "    # Using the filtered and properly structured messages\n",
    "    try:\n",
    "        response = data_llm_with_tools.invoke(filtered_messages)\n",
    "        return {\"messages\": [response]}\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Logging the error and provide a fallback response\n",
    "        print(f\"Error in data_chatbot_node: {e}\")\n",
    "        # Print filtered messages for debugging\n",
    "        for i, msg in enumerate(filtered_messages):\n",
    "            print(f\"Message {i}: {type(msg).__name__} - {msg}\")\n",
    "        \n",
    "        # Returning a fallback AI message\n",
    "        return {\"messages\": [AIMessage(content=\"I encountered an error processing your request. Please try again with a different query.\")]}\n",
    "\n",
    "def get_state(state: dict) -> str:\n",
    "    \"\"\"Determines the next step in the workflow.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "    \n",
    "        if any(call[\"name\"] == \"execute_query_tool\" for call in last_message.tool_calls):\n",
    "            return \"execute_sql\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the workflow graph with improved message handling\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"get_schema\", get_schema_node)\n",
    "workflow.add_node(\"data_chatbot\", data_chatbot_node)\n",
    "\n",
    "# Using a custom function to handle the tool execution and ensure proper message formatting\n",
    "def execute_sql_with_formatting(state):\n",
    "    \"\"\"Execute SQL and ensure proper message formatting for the response\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if not hasattr(last_message, 'tool_calls'):\n",
    "        # If there's no tool call in the last message, return early\n",
    "        return {\"messages\": messages + [AIMessage(content=\"No valid tool call found.\")]}\n",
    "    \n",
    "    # Finding the execute_query_tool call\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        \n",
    "        if tool_call[\"name\"] == \"execute_query_tool\":\n",
    "        \n",
    "            # Executing the query\n",
    "            query = tool_call[\"args\"][\"query\"]\n",
    "            try:\n",
    "                result = execute_query_tool(query)\n",
    "                \n",
    "                # Creating a properly formatted tool response\n",
    "                from langchain_core.messages import ToolMessage\n",
    "                \n",
    "                tool_message = ToolMessage(\n",
    "                    content=result,\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                    name=\"execute_query_tool\"\n",
    "                )\n",
    "                return {\"messages\": messages + [tool_message]}\n",
    "            except Exception as e:\n",
    "                return {\"messages\": messages + [AIMessage(content=f\"Error executing query: {str(e)}\")]}\n",
    "    \n",
    "    # If no execute_query_tool call was found\n",
    "    return {\"messages\": messages + [AIMessage(content=\"No execute_query_tool call found.\")]}\n",
    "\n",
    "workflow.add_node(\"execute_sql\", execute_sql_with_formatting)\n",
    "workflow.add_edge(START, \"get_schema\")\n",
    "workflow.add_edge(\"get_schema\", \"data_chatbot\")\n",
    "workflow.add_conditional_edges(\"data_chatbot\", get_state, [\"execute_sql\", END])\n",
    "workflow.add_edge(\"execute_sql\", \"data_chatbot\")\n",
    "\n",
    "# Compiling the workflow graph\n",
    "data_chatbot_graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"101\"}, \"recursion_limit\": 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Schema retrieved from memory.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  execute_query_tool (call_lYMuxdPSAtMIc2sW1ACHb9aF)\n",
      " Call ID: call_lYMuxdPSAtMIc2sW1ACHb9aF\n",
      "  Args:\n",
      "    query: SELECT * FROM menu LIMIT 5\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: execute_query_tool\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"menu_id\": 15,\n",
      "    \"menu_name\": \"Lavender Sea Salt Chocolate\",\n",
      "    \"menu_description\": \"A decadent blend of rich dark chocolate infused with aromatic lavender, topped with a sprinkle of delicate sea salt.\",\n",
      "    \"menu_size\": \"1 pc\",\n",
      "    \"menu_price\": 5.5,\n",
      "    \"menu_allergy_info\": \"Contains: Milk, Soy, Gluten. May contain traces of nuts.\",\n",
      "    \"menu_image_gcs_filename\": \"gs://data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/15.png\",\n",
      "    \"menu_image_http_url\": \"https://storage.googleapis.com/data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/15.png\",\n",
      "    \"menu_image_prompt\": \"Imagine a single, perfectly formed chocolate square, dusted with a light sprinkling of sea salt. The chocolate is a deep, rich brown, almost black, and has a slight sheen to it. A sprig of lavender lies next to the chocolate square, its purple flowers contrasting beautifully with the dark chocolate. The scene is softly lit, creating a mood of luxury and indulgence.\"\n",
      "  },\n",
      "  {\n",
      "    \"menu_id\": 252,\n",
      "    \"menu_name\": \"Tour de France Commemorative Chocolate - Modern\",\n",
      "    \"menu_description\": \"Crafted with the finest cocoa beans and a touch of sweetness, this bar offers a classic chocolate experience with a sophisticated twist. The elegant wrapper, adorned with a gold foil bicycle and forest scene, hints at the adventurous spirit within each bite.\",\n",
      "    \"menu_size\": \"1 pc\",\n",
      "    \"menu_price\": 14.99,\n",
      "    \"menu_allergy_info\": \"Contains: Gluten, Milk, Soy\",\n",
      "    \"menu_image_gcs_filename\": \"gs://data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/252.jpg\",\n",
      "    \"menu_image_http_url\": \"https://storage.googleapis.com/data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/252.jpg\",\n",
      "    \"menu_image_prompt\": \"Bike-themed chocolate bar with a modern aesthetic commemorating the Tour De France 2025\"\n",
      "  },\n",
      "  {\n",
      "    \"menu_id\": 253,\n",
      "    \"menu_name\": \"Tour de France Commemorative Chocolate - Modern\",\n",
      "    \"menu_description\": \"This bar features a beautifully molded vintage bicycle design, making each piece a miniature work of art.  Crafted with the finest cacao beans, this chocolate delivers a deep, complex flavor that will satisfy any chocolate connoisseur.\",\n",
      "    \"menu_size\": \"1 pc\",\n",
      "    \"menu_price\": 14.99,\n",
      "    \"menu_allergy_info\": \"Contains: Gluten, Milk, Soy\",\n",
      "    \"menu_image_gcs_filename\": \"gs://data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/253.jpg\",\n",
      "    \"menu_image_http_url\": \"https://storage.googleapis.com/data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/253.jpg\",\n",
      "    \"menu_image_prompt\": \"Bike-themed chocolate bar with a vintage aesthetic commemorating the Tour De France 2025\"\n",
      "  },\n",
      "  {\n",
      "    \"menu_id\": 251,\n",
      "    \"menu_name\": \"Tour de France Commemorative Chocolate - Steampunk\",\n",
      "    \"menu_description\": \"A decadent treat for any chocolate lover, commemorating the Tour de France 2025. Made with the finest ingredients, this chocolate bar is rich, smooth, and intensely flavorful. The elegant box features a striking gold foil design, making it the perfect gift for any occasion.\",\n",
      "    \"menu_size\": \"1 pc\",\n",
      "    \"menu_price\": 14.99,\n",
      "    \"menu_allergy_info\": \"Contains: Gluten, Milk, Soy\",\n",
      "    \"menu_image_gcs_filename\": \"gs://data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/251.jpg\",\n",
      "    \"menu_image_http_url\": \"https://storage.googleapis.com/data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/251.jpg\",\n",
      "    \"menu_image_prompt\": \"Bike-themed chocolate bar with a steampunk aesthetic commemorating the Tour De France 2025\"\n",
      "  },\n",
      "  {\n",
      "    \"menu_id\": 124,\n",
      "    \"menu_name\": \"Parisian Chocolate Easter Egg\",\n",
      "    \"menu_description\": \"A blend of dark and white chocolate formed into a hollow egg. Filled with a variety or truffles.\",\n",
      "    \"menu_size\": \"1 egg\",\n",
      "    \"menu_price\": 24.99,\n",
      "    \"menu_allergy_info\": \"Contains dairy and soy.  May contain traces of nuts.\",\n",
      "    \"menu_image_gcs_filename\": \"gs://data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/124.png\",\n",
      "    \"menu_image_http_url\": \"https://storage.googleapis.com/data-analytics-golden-demo/chocolate-ai/v1/Synthetic-Data-Generation-Menu/124.png\",\n",
      "    \"menu_image_prompt\": \"A partially opened Easter egg made of dark and white chocolate. The egg is sitting on a small plate and in the background is a city view of Paris at night. There are truffles filling the egg.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16488\\105454289.py:22: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = execute_query_tool(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  SubmitFinalAnswer (call_Z3dni21VBIE7wUJMQVBWys6f)\n",
      " Call ID: call_Z3dni21VBIE7wUJMQVBWys6f\n",
      "  Args:\n",
      "    final_answer: Here are 5 menu items:\n",
      "1. Lavender Sea Salt Chocolate - $5.5\n",
      "2. Tour de France Commemorative Chocolate - Modern - $14.99\n",
      "3. Tour de France Commemorative Chocolate - Modern - $14.99\n",
      "4. Tour de France Commemorative Chocolate - Steampunk - $14.99\n",
      "5. Parisian Chocolate Easter Egg - $24.99\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Schema retrieved from memory.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  execute_query_tool (call_jWPDCKx7tjF8GQTIGmklQLT1)\n",
      " Call ID: call_jWPDCKx7tjF8GQTIGmklQLT1\n",
      "  Args:\n",
      "    query: SELECT menu_name, menu_price AS price FROM menu ORDER BY menu_price DESC LIMIT 1\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: execute_query_tool\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"menu_name\": \"Five Spice Chocolate Celebration Cake\",\n",
      "    \"price\": 75.0\n",
      "  }\n",
      "]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  SubmitFinalAnswer (call_eoIGqKcUZxgzQJ2i22ymTz0B)\n",
      " Call ID: call_eoIGqKcUZxgzQJ2i22ymTz0B\n",
      "  Args:\n",
      "    final_answer: The most expensive item on the menu is the Five Spice Chocolate Celebration Cake priced at $75.0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User (q/Q to quit): \")\n",
    "    if user_input in {\"q\", \"Q\"}:\n",
    "        break\n",
    "    for output in data_chatbot_graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config=config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
